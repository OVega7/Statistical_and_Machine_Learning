{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Binary - Supervised)\n",
    "\n",
    "We want to predict whether or not a student will be admitted into University A based off of ther GMAT score and their GPA. \n",
    "\n",
    "We will train our model to find the probabily that a student will be admitted.\n",
    "\n",
    "### Feed Forward (with a single neuron)\n",
    "\n",
    "$(x^i, y^i),..., (x^n,y^n)$\n",
    "\n",
    "Feature Vector  \n",
    "$x^i = Student_i\\ GMAT Score  \\\\\n",
    "      Student_i\\ GPA$\n",
    " \n",
    "Label  \n",
    "$y^i = {0,1}$  \n",
    "where 1 indicates student_i was accepted and 0 if not.\n",
    "\n",
    "=== Figure out how to do neural map on latex for jupyter ===\n",
    "\n",
    "We have feature vectors ($x^i$) and weights ($\\omega$) give a linear combinations $Z = \\omega^T x^i = b$ imply an activation funciton ($\\sigma = \\frac{1}{1+e^-x}$) of our choice to get an output ($\\hat y^i = \\sigma(Z^i)$).\n",
    "\n",
    "\n",
    "---\n",
    "#### Loss Cross Entropy Function\n",
    "We want $L(\\hat y, j^i) =$ How close $\\hat y^i$ is to $y^i$  \n",
    "First consder maximizing $P(y^i|x^i)$ the probability that $\\hat y^i$ predicts $y^i$. Since there are only two discrete outputs, this is subject to the following formula by $Bernoulli$. \n",
    "\n",
    "Maximize -> $P(y^i|x^i) = \\hat y^y (1-\\hat y)^{1-y}$\n",
    "equivalent to:  \n",
    "$log(P(y^i|x^i)) = log(\\hat y^y (1-\\hat y)^{1-y})$\n",
    "$log(P(y^i|x^i)) = ylog \\hat y + (1-y)log(1- \\hat y)$\n",
    "\n",
    "For gradient descent we want a minimization problem:  \n",
    "Loss Cross Entropy Function  $L_{CE}(\\omega, b)$  \n",
    "Minimize  \n",
    "$-log(P(y^i|x^i)) = -[ylog \\hat y + (1-y)log(1- \\hat y)]$\n",
    "\n",
    "$ -log(P(y^i|x^i)) = -[ylog \\sigma (z) + (1-y)log(1- \\sigma(z))]$\n",
    "\n",
    "$ -log(P(y^i|x^i)) = -[ylog \\sigma (\\omega^Tx+b) + (1-y)log(1- \\sigma(\\omega^Tx+b))]$\n",
    "\n",
    "---\n",
    "---\n",
    "#### Train (Stochastic Gradient Descent)\n",
    "\n",
    "We want $\\hat\\theta = argminL_{CE}(x_i,y:\\theta)$, where $\\theta = \\omega, b$\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial L_{CE}}{\\partial \\omega_j}(\\omega, b) = [\\theta(\\omega x+b)-y]\\cdot x_j$  \n",
    "\n",
    "$\\frac{\\partial L_{CE}}{\\partial b}(\\omega, b)= \\theta(\\omega^T x+b)-y$\n",
    "\n",
    "For a given $(x^i, y)$  \n",
    "$\\omega_j^{k+1} = \\omega_j^k - \\alpha \\cdot \\frac{\\partial L_{CE}}{\\partial \\omega_j^k}(\\omega^k, b^k)$ for $j = 1, 2$\n",
    "\n",
    "$b^{k+1} = b^k - \\alpha \\cdot \\frac{\\partial L_{CE}}{\\partial b}(\\omega^k, b^k)$\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>40 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>690</td><td>1.7</td><td>1</td><td>0</td></tr><tr><th>11</th><td>610</td><td>2.7</td><td>3</td><td>0</td></tr><tr><th>12</th><td>690</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>13</th><td>710</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>14</th><td>680</td><td>3.3</td><td>4</td><td>0</td></tr><tr><th>15</th><td>770</td><td>3.3</td><td>3</td><td>1</td></tr><tr><th>16</th><td>610</td><td>3.0</td><td>1</td><td>0</td></tr><tr><th>17</th><td>580</td><td>2.7</td><td>4</td><td>0</td></tr><tr><th>18</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>19</th><td>540</td><td>2.7</td><td>2</td><td>0</td></tr><tr><th>20</th><td>590</td><td>2.3</td><td>3</td><td>0</td></tr><tr><th>21</th><td>620</td><td>3.3</td><td>2</td><td>1</td></tr><tr><th>22</th><td>600</td><td>2.0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>550</td><td>2.3</td><td>4</td><td>0</td></tr><tr><th>24</th><td>550</td><td>2.7</td><td>1</td><td>0</td></tr><tr><th>25</th><td>570</td><td>3.0</td><td>2</td><td>0</td></tr><tr><th>26</th><td>670</td><td>3.3</td><td>6</td><td>1</td></tr><tr><th>27</th><td>660</td><td>3.7</td><td>4</td><td>1</td></tr><tr><th>28</th><td>580</td><td>2.3</td><td>2</td><td>0</td></tr><tr><th>29</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>30</th><td>660</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\t11 & 610 & 2.7 & 3 & 0 \\\\\n",
       "\t12 & 690 & 3.7 & 5 & 1 \\\\\n",
       "\t13 & 710 & 3.7 & 6 & 1 \\\\\n",
       "\t14 & 680 & 3.3 & 4 & 0 \\\\\n",
       "\t15 & 770 & 3.3 & 3 & 1 \\\\\n",
       "\t16 & 610 & 3.0 & 1 & 0 \\\\\n",
       "\t17 & 580 & 2.7 & 4 & 0 \\\\\n",
       "\t18 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t19 & 540 & 2.7 & 2 & 0 \\\\\n",
       "\t20 & 590 & 2.3 & 3 & 0 \\\\\n",
       "\t21 & 620 & 3.3 & 2 & 1 \\\\\n",
       "\t22 & 600 & 2.0 & 1 & 0 \\\\\n",
       "\t23 & 550 & 2.3 & 4 & 0 \\\\\n",
       "\t24 & 550 & 2.7 & 1 & 0 \\\\\n",
       "\t25 & 570 & 3.0 & 2 & 0 \\\\\n",
       "\t26 & 670 & 3.3 & 6 & 1 \\\\\n",
       "\t27 & 660 & 3.7 & 4 & 1 \\\\\n",
       "\t28 & 580 & 2.3 & 2 & 0 \\\\\n",
       "\t29 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t30 & 660 & 3.3 & 5 & 1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "40×4 DataFrame\n",
       "│ Row │ gmat  │ gpa     │ work_experience │ admitted │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m           │ \u001b[90mInt64\u001b[39m    │\n",
       "├─────┼───────┼─────────┼─────────────────┼──────────┤\n",
       "│ 1   │ 780   │ 4.0     │ 3               │ 1        │\n",
       "│ 2   │ 750   │ 3.9     │ 4               │ 1        │\n",
       "│ 3   │ 690   │ 3.3     │ 3               │ 0        │\n",
       "│ 4   │ 710   │ 3.7     │ 5               │ 1        │\n",
       "│ 5   │ 680   │ 3.9     │ 4               │ 0        │\n",
       "│ 6   │ 730   │ 3.7     │ 6               │ 1        │\n",
       "│ 7   │ 690   │ 2.3     │ 1               │ 0        │\n",
       "│ 8   │ 720   │ 3.3     │ 4               │ 1        │\n",
       "│ 9   │ 740   │ 3.3     │ 5               │ 1        │\n",
       "│ 10  │ 690   │ 1.7     │ 1               │ 0        │\n",
       "⋮\n",
       "│ 30  │ 660   │ 3.3     │ 5               │ 1        │\n",
       "│ 31  │ 640   │ 3.0     │ 1               │ 0        │\n",
       "│ 32  │ 620   │ 2.7     │ 2               │ 0        │\n",
       "│ 33  │ 660   │ 4.0     │ 4               │ 1        │\n",
       "│ 34  │ 660   │ 3.3     │ 6               │ 1        │\n",
       "│ 35  │ 680   │ 3.3     │ 5               │ 1        │\n",
       "│ 36  │ 650   │ 2.3     │ 1               │ 0        │\n",
       "│ 37  │ 670   │ 2.7     │ 2               │ 0        │\n",
       "│ 38  │ 580   │ 3.3     │ 1               │ 0        │\n",
       "│ 39  │ 590   │ 1.7     │ 4               │ 0        │\n",
       "│ 40  │ 690   │ 3.7     │ 5               │ 1        │"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "data = DataFrame(CSV.File(\"candidates_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[x[1], x[2]] for x in zip(data.gmat, data.gpa)]\n",
    "y_data = [x for x in data.admitted];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "σ (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1/(1+exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cross_entropy_loss(x, y, w, b)\n",
    "    return -y*log(σ(w'x + b)) - (1-y)*log(1 - σ(w'x+b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What we want to minimize\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return(1/N)*sum([cross_entropy_loss(features[i], labels[i], w, b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features, labels, w, b, α)\n",
    "    \n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    \n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i]+b)-labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i]+b)-labels[i])\n",
    "    end\n",
    "    w = w -α*del_w\n",
    "    b = b -α*del_b\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599451\n",
      "The new cost is: 0.6931188566349795\n"
     ]
    }
   ],
   "source": [
    "w = [0.0, 0.0]\n",
    "b = 0.0\n",
    "\n",
    "println(\"The initial cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, 0.0000001)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_batch_gradient_descent(features, labels, w, b, α, epochs)\n",
    "    for i = 1:epochs\n",
    "        #Updating w,b\n",
    "        w,b = batch_gradient_descent(features, labels, w, b, α)\n",
    "        \n",
    "        if i == 1\n",
    "            println(\"Epoch \", i, \" with cost: \" , average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        if i == 100\n",
    "            println(\"Epoch \", i, \" with cost: \" , average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        if i == 1000\n",
    "            println(\"Epoch \", i, \" with cost: \" , average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        if i == 10000\n",
    "            println(\"Epoch \", i, \" with cost: \" , average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        if i == 100000\n",
    "            println(\"Epoch \", i, \" with cost: \" , average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "    end\n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.6931188566349795\n",
      "Epoch 100 with cost: 0.6930977152288289\n",
      "Epoch 1000 with cost: 0.6930282266294219\n",
      "Epoch 10000 with cost: 0.6923351299473173\n",
      "Epoch 100000 with cost: 0.6855799117618873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.0, 0.0]\n",
    "b = 0.0\n",
    "w,b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.632691827205463\n",
      "Epoch 100 with cost: 0.6326872176867723\n",
      "Epoch 1000 with cost: 0.6326453230745035\n",
      "Epoch 10000 with cost: 0.6322273761510574\n",
      "Epoch 100000 with cost: 0.6281458496177578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0036370472006028087, 0.8445273524582464], -0.22916112906791533)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.5954226371160484\n",
      "Epoch 100 with cost: 0.5954197027866317\n",
      "Epoch 1000 with cost: 0.5953930326541702\n",
      "Epoch 10000 with cost: 0.5951268841889991\n",
      "Epoch 100000 with cost: 0.592519647987565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.004865616495539809, 1.13654419765688], -0.33931275004419825)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.5709852048107705\n",
      "Epoch 100 with cost: 0.5709832149786357\n",
      "Epoch 1000 with cost: 0.5709651288345847\n",
      "Epoch 10000 with cost: 0.5707845878223877\n",
      "Epoch 100000 with cost: 0.5690106767456337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.005840241274094449, 1.3738471774117296], -0.44718732969308767)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.49662146605673574\n",
      "Epoch 100 with cost: 0.4966198632622398\n",
      "Epoch 1000 with cost: 0.49660529525590635\n",
      "Epoch 10000 with cost: 0.4964598968477649\n",
      "Epoch 100000 with cost: 0.4950330686603879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.009966727824506375, 2.6569769985318787], -1.822723496544359)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000005, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x, y, w, b)\n",
    "    if σ(w'x+b)>=.5\n",
    "        println(\"Predicted to be Accepted\")\n",
    "        y==1 ? println(\"Accepted\") : println(\"NOT Accepted\")\n",
    "    else\n",
    "        println(\"Predicted NOT be Accepted\")\n",
    "        y==1 ? println(\"Accepted\") : println(\"NOT Accepted\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted NOT be Accepted\n",
      "NOT Accepted\n",
      "\n",
      "Predicted to be Accepted\n",
      "Accepted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i = 1:length(x_data)\n",
    "    predict(x_data[i], y_data[i], w, b)\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.225"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "\n",
    "print(mean_error/length(x_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
